{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0AkBsSjClPMg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\ons\\anaconda3\\lib\\site-packages (2.6.0+cpu)\n",
      "Requirement already satisfied: filelock in c:\\users\\ons\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\ons\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ons\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ons\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\ons\\anaconda3\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\ons\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\ons\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ons\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.18.0\n",
      "Uninstalling tensorflow-2.18.0:\n",
      "  Successfully uninstalled tensorflow-2.18.0\n",
      "Found existing installation: tensorflow_intel 2.18.0\n",
      "Uninstalling tensorflow_intel-2.18.0:\n",
      "  Successfully uninstalled tensorflow_intel-2.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Ons\\anaconda3\\Lib\\site-packages\\~ensorflow'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Skipping tensorflow-gpu as it is not installed.\n",
      "WARNING: Skipping tensorflow-cpu as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall tensorflow tensorflow-intel tensorflow-gpu tensorflow-cpu -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ Fine-tuning Flan-T5 pour l'expansion d'abr√©viations m√©dicales\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl.metadata (8.3 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/992.0 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 262.1/992.0 kB ? eta -:--:--\n",
      "   ------------------------------- -------- 786.4/992.0 kB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 992.0/992.0 kB 1.7 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae21b93bd054174829802d0aa5ad33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ons\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ons\\.cache\\huggingface\\hub\\models--google--flan-t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1d96fa83b94a47b6cee232307f804b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022a8665ab6848cc9bc6f3afeb2e67a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e614b02f96234a40ac6cf8ce37478826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a40390912f4bf78d0f64fe5f6ea266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c00fffa35934ee68090969d5a724735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8197ec0cda4156b0188a5bd3dcfa4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# 1. Charger le mod√®le Flan-T5 (taille small)\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[hf_xet] in c:\\users\\ons\\anaconda3\\lib\\site-packages (0.30.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\ons\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ons\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.13.2)\n",
      "Collecting hf-xet>=0.1.4 (from huggingface_hub[hf_xet])\n",
      "  Downloading hf_xet-1.0.3-cp37-abi3-win_amd64.whl.metadata (498 bytes)\n",
      "Requirement already satisfied: colorama in c:\\users\\ons\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub[hf_xet]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2025.1.31)\n",
      "Downloading hf_xet-1.0.3-cp37-abi3-win_amd64.whl (4.1 MB)\n",
      "   ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.5/4.1 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.0/4.1 MB 2.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 1.6/4.1 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.8/4.1 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.1/4.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 2.6/4.1 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 3.1/4.1 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.4/4.1 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.9/4.1 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.1/4.1 MB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: hf-xet\n",
      "Successfully installed hf-xet-1.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub[hf_xet]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Charger le fichier nettoy√©\n",
    "df = pd.read_csv(r\"C:\\Users\\Ons\\Downloads\\data_abbreviations.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Cr√©er un Dataset Hugging Face\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b331576c64e544b087f4bff4504db8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "def preprocess(example):\n",
    "    if not example.get(\"input\") or not example.get(\"target\"):\n",
    "        return {}  # ignorer si champ vide\n",
    "\n",
    "    encoded = tokenizer(\n",
    "        text=example[\"input\"],\n",
    "        text_target=example[\"target\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )\n",
    "    encoded[\"labels\"] = encoded[\"labels\"]\n",
    "    return encoded\n",
    "\n",
    "# Appliquer la fonction\n",
    "tokenized_dataset = dataset.map(preprocess, remove_columns=dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate>=0.26.0\n",
      "  Using cached accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\ons\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ons\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (2.6.0+cpu)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\ons\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ons\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\ons\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ons\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ons\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate>=0.26.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2025.1.31)\n",
      "Using cached accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install \"accelerate>=0.26.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./flan_t5_abbr_model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    save_total_limit=1,\n",
    "    save_steps=20,\n",
    "    logging_steps=5,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 08:04, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>28.091400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>26.529100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>21.573700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>20.018400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>18.673800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>17.741000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>17.477800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>14.893300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>14.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>13.089100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>12.077800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>11.197700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>10.327100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>8.670400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>8.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>7.867000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>7.410400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>6.763600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>6.782000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.526800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>6.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>5.955900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>6.018400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>5.739800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>5.582100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>5.497100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>5.449300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>5.390300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>5.351300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.189300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>5.288300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>5.081500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>5.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>5.098600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>5.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>5.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>5.098600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>4.943900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>5.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.937900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=9.633376989364624, metrics={'train_runtime': 488.0776, 'train_samples_per_second': 3.176, 'train_steps_per_second': 0.41, 'total_flos': 36016285286400.0, 'train_loss': 9.633376989364624, 'epoch': 10.0})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_abbreviation(abbr):\n",
    "    inputs = tokenizer(abbr, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=64)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S ‚Üí \n",
      "C ‚Üí - - - - -\n",
      "RN ‚Üí e-mail\n",
      "SF ‚Üí Materials and & Equipment Manufacturers\n"
     ]
    }
   ],
   "source": [
    "# Exemples\n",
    "for abbr in [\"S\",\"C\", \"RN\", \"SF\"]:\n",
    "    print(f\"{abbr} ‚Üí {expand_abbreviation(abbr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    r\"C:\\Users\\Ons\\Downloads\\data_abbreviations_enrichi.csv\",\n",
    "    engine=\"python\",\n",
    "    encoding=\"latin1\",\n",
    "    sep=\",\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"input\"] = df[\"input\"].apply(lambda x: x if x.startswith(\"expand:\") else \"expand: \" + x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data_abbreviations.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  input                                        target\n",
      "0   CSC           Consultation specifique au cabinet \n",
      "1   CSC            Rendez-vous specifique au cabinet \n",
      "2   CSC  Consultation specifique en cabinet m√É¬©dical \n",
      "3   IFD          Indemnite forfaitaire de deplacement\n",
      "4   IFD          Indemnite forfaitaire de deplacement\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Utilise ISO-8859-1 ou windows-1252 si utf-8 √©choue\n",
    "df = pd.read_csv(r\"C:\\Users\\Ons\\Downloads\\data_abbreviations_augmented.csv\", encoding=\"ISO-8859-1\")  # ou encoding=\"windows-1252\"\n",
    "\n",
    "# V√©rifie que le contenu est bien charg√©\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion en dataset Hugging Face\n",
    "dataset = Dataset.from_pandas(df[[\"input\", \"target\"]])\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Charger le tokenizer et le mod√®le T5\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a891e6255cd4541a5c6f8b6fdeb94f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/372 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed91ab3140c446db1798e87b3180fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(example):\n",
    "    inputs = tokenizer(example[\"input\"], truncation=True, padding=\"max_length\", max_length=32)\n",
    "    targets = tokenizer(example[\"target\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\ons\\anaconda3\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\ons\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\ons\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ons\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ons\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version de transformers utilis√©e : 4.51.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(\"Version de transformers utilis√©e :\", transformers.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ons\\AppData\\Local\\Temp\\ipykernel_4444\\2306116019.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# ‚úÖ Forcer √† utiliser le bon TrainingArguments depuis transformers\n",
    "TrainingArguments = transformers.TrainingArguments\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"t5_abbr_expander\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='470' max='470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [470/470 15:19, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>8.854800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.407200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.838000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.308600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.959200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.772900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.595100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.545400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.474200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=470, training_loss=2.9074879260773354, metrics={'train_runtime': 922.1749, 'train_samples_per_second': 4.034, 'train_steps_per_second': 0.51, 'total_flos': 31466968842240.0, 'train_loss': 2.9074879260773354, 'epoch': 10.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Entra√Ænement du mod√®le\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('t5_abbreviation_expander\\\\tokenizer_config.json',\n",
       " 't5_abbreviation_expander\\\\special_tokens_map.json',\n",
       " 't5_abbreviation_expander\\\\spiece.model',\n",
       " 't5_abbreviation_expander\\\\added_tokens.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Sauvegarde du mod√®le fine-tun√©\n",
    "model.save_pretrained(\"t5_abbreviation_expander\")\n",
    "tokenizer.save_pretrained(\"t5_abbreviation_expander\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# üîê Fixer les seeds pour la reproductibilit√©\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# üîí Rendre l'entra√Ænement et l'inf√©rence 100% d√©terministes\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_abbreviation(abbr):\n",
    "    # üîí Met le mod√®le en mode √©valuation (important)\n",
    "    model.eval()\n",
    "    \n",
    "    # Pr√©parer le prompt\n",
    "    prompt = \"expand: \" + abbr\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # G√©n√©ration d√©terministe : pas de sampling, pas de beam search\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=50,\n",
    "        num_beams=1,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    # Retourner le texte g√©n√©r√©\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTA ‚Üí \n",
      "CRP ‚Üí \n",
      "DC ‚Üí \n",
      "CSC ‚Üí \n",
      "V ‚Üí \n",
      "D ‚Üí \n"
     ]
    }
   ],
   "source": [
    "for abbr in [\"HTA\", \"CRP\", \"DC\", \"CSC\", \"V\", \"D\"]:\n",
    "    print(f\"{abbr} ‚Üí {expand_abbreviation(abbr)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v‚Üí \n",
      "DC ‚Üí \n"
     ]
    }
   ],
   "source": [
    "# 7. Exemples de test\n",
    "print(\"v‚Üí\", expand_abbreviation(\"V\"))\n",
    "print(\"DC ‚Üí\", expand_abbreviation(\"DC\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DC ‚Üí Actes de radiodiagnostic\n",
      "CSC ‚Üí Acte de chirurgie et de chirurgie\n",
      "V ‚Üí Visite a domicile a domicile\n",
      "D‚Üí Actes de kinesitherapeutie\n"
     ]
    }
   ],
   "source": [
    "print(\"DC ‚Üí\", expand_abbreviation(\"DC\"))\n",
    "print(\"CSC ‚Üí\", expand_abbreviation(\"CSC\"))\n",
    "print(\"V ‚Üí\", expand_abbreviation(\"V\"))\n",
    "print(\"D‚Üí\", expand_abbreviation(\"IK\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Charger le fichier CSV (remplace par ton chemin local si besoin)\n",
    "df = pd.read_csv(r\"C:\\Users\\Ons\\Downloads\\data_abbreviations_enrichi_3x_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Renommer les colonnes pour qu'elles soient compatibles\n",
    "raw_dataset = Dataset.from_pandas(df.rename(columns={\"input\": \"input_text\", \"target\": \"target_text\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Charger le tokenizer et le mod√®le FLAN-T5\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Charger le fichier CSV (remplace par ton chemin local si besoin)\n",
    "df = pd.read_csv(r\"C:\\Users\\Ons\\Downloads\\data_abbreviations_enrichi_9x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433b003129de49af8d0b6d87982d3ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/327 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. Tokenizer la data\n",
    "def preprocess(example):\n",
    "    model_input = tokenizer(example[\"input_text\"], padding=\"max_length\", truncation=True, max_length=32)\n",
    "    labels = tokenizer(text_target=example[\"target_text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "    model_input[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_input\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Configuration d'entra√Ænement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"flan-t5-finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=9,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='369' max='369' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [369/369 1:05:55, Epoch 9/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.753600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.605300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.495800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.480800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.462500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.416200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.368400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.369800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.349700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.359500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.328800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.282400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.296900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.256800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.260400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.250400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.224200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.249800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.234400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.192300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.170900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.232900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.200900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.175800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.195200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.173700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.191600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.159700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.182700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=369, training_loss=0.30874759330335994, metrics={'train_runtime': 3965.9772, 'train_samples_per_second': 0.742, 'train_steps_per_second': 0.093, 'total_flos': 34192234708992.0, 'train_loss': 0.30874759330335994, 'epoch': 9.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Entra√Ænement\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('flan-t5-finetuned\\\\tokenizer_config.json',\n",
       " 'flan-t5-finetuned\\\\special_tokens_map.json',\n",
       " 'flan-t5-finetuned\\\\spiece.model',\n",
       " 'flan-t5-finetuned\\\\added_tokens.json',\n",
       " 'flan-t5-finetuned\\\\tokenizer.json')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Sauvegarder le mod√®le\n",
    "model.save_pretrained(\"flan-t5-finetuned\")\n",
    "tokenizer.save_pretrained(\"flan-t5-finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Exemple de pr√©diction\n",
    "def predict_abbreviation(abbr):\n",
    "    inputs = tokenizer(abbr, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acte de prelevement de produits biologiques biologiques\n"
     ]
    }
   ],
   "source": [
    "# Exemple :\n",
    "print(predict_abbreviation(\"APB\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "APB      36\n",
      "E        27\n",
      "KH       27\n",
      "RT       27\n",
      "VN       27\n",
      "RN       27\n",
      "VNPSY    18\n",
      "RD       18\n",
      "V        18\n",
      "SF       18\n",
      "AMO      18\n",
      "AMI      18\n",
      "KE       18\n",
      "AMY      18\n",
      "SCM      18\n",
      "KFC      18\n",
      "S        18\n",
      "B        18\n",
      "CSF      18\n",
      "IK        9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Afficher les 20 abr√©viations les plus fr√©quentes\n",
    "print(df[\"input\"].value_counts().head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "APB     36\n",
      "E       27\n",
      "KH      27\n",
      "RT      27\n",
      "VN      27\n",
      "        ..\n",
      "ORT      9\n",
      "ZN       9\n",
      "AMS      9\n",
      "Z        9\n",
      "KC60     9\n",
      "Name: count, Length: 83, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Si ta colonne s‚Äôappelle toujours \"input\"\n",
    "counts =df[\"input\"].value_counts()\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant √©quilibrage :\n",
      "input\n",
      "Z1      9\n",
      "KC      9\n",
      "CALD    9\n",
      "P       9\n",
      "SPR     9\n",
      "Name: count, dtype: int64\n",
      "input\n",
      "APB    36\n",
      "KH     27\n",
      "RT     27\n",
      "VN     27\n",
      "RN     27\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Apr√®s √©quilibrage :\n",
      "input\n",
      "KC8    20\n",
      "AMM    20\n",
      "AMO    20\n",
      "AMP    20\n",
      "AMS    20\n",
      "Name: count, dtype: int64\n",
      "input\n",
      "APB    30\n",
      "KH     27\n",
      "RN     27\n",
      "RT     27\n",
      "E      27\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset √©quilibr√© enregistr√© sous data_abbreviations_balanced.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Charge ton dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\Ons\\Downloads\\data_abbreviations_enrichi_9x.csv\")\n",
    "\n",
    "# 2. Affiche la r√©partition initiale\n",
    "print(\"Avant √©quilibrage :\")\n",
    "print(df[\"input\"].value_counts().sort_values().head(5))\n",
    "print(df[\"input\"].value_counts().sort_values(ascending=False).head(5))\n",
    "\n",
    "# 3. D√©finis tes seuils\n",
    "min_count = 20   # nombre mini d‚Äôexemples par abbr\n",
    "max_count = 30   # nombre maxi d‚Äôexemples par abbr\n",
    "\n",
    "# 4. Undersampling des abbr trop fr√©quentes\n",
    "frames = []\n",
    "for abbr, grp in df.groupby(\"input\"):\n",
    "    if len(grp) > max_count:\n",
    "        frames.append(grp.sample(n=max_count, random_state=42))\n",
    "    else:\n",
    "        frames.append(grp)\n",
    "df_step1 = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# 5. Oversampling des abbr trop rares\n",
    "frames = [df_step1]\n",
    "counts = df_step1[\"input\"].value_counts()\n",
    "for abbr, cnt in counts.items():\n",
    "    if cnt < min_count:\n",
    "        needed = min_count - cnt\n",
    "        block = df_step1[df_step1[\"input\"] == abbr]\n",
    "        dup = block.sample(n=needed, replace=True, random_state=42)\n",
    "        frames.append(dup)\n",
    "df_balanced = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# 6. V√©rifie la nouvelle r√©partition\n",
    "print(\"\\nApr√®s √©quilibrage :\")\n",
    "print(df_balanced[\"input\"].value_counts().sort_values().head(5))\n",
    "print(df_balanced[\"input\"].value_counts().sort_values(ascending=False).head(5))\n",
    "\n",
    "# 7. Sauvegarde\n",
    "df_balanced.to_csv(r\"C:\\Users\\Ons\\Downloads\\data_abbreviations_balanced.csv\", index=False)\n",
    "print(\"\\nDataset √©quilibr√© enregistr√© sous data_abbreviations_balanced.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Charger le fichier CSV (remplace par ton chemin local si besoin)\n",
    "df = pd.read_csv(r\"C:\\Users\\Ons\\Downloads\\data_abbreviations_balanced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Renommer les colonnes pour qu'elles soient compatibles\n",
    "raw_dataset = Dataset.from_pandas(df.rename(columns={\"input\": \"input_text\", \"target\": \"target_text\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Charger le tokenizer et le mod√®le FLAN-T5\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1629bf96bea647a8b3fc0b29b238b76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1705 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. Tokenizer la data\n",
    "def preprocess(example):\n",
    "    model_input = tokenizer(example[\"input_text\"], padding=\"max_length\", truncation=True, max_length=32)\n",
    "    labels = tokenizer(text_target=example[\"target_text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "    model_input[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_input\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Configuration d'entra√Ænement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"flan-t5-finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=6,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 3:33:39, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>23.317400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>11.178200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>6.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.958800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.061500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.142800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.361400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.718100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.334000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.061300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.938200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.846600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.706800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.595000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.510600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.464400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.478000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.353300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.394700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.311100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.282100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.262200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.240700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.275100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.200300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.214600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.175100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.149400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.138900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.136100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.102500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.102800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.113800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.092200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.085500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.070100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.065400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.087500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.080700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.068700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.060500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.070500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.070500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.064400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.053200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.044600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.052200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.036100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.032900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.040400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.047700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.038700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.039100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.036900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.040400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.036800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.031100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.038400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.028900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.024100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.023200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.026200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.023900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.020700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.021600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.026200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.016600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.022300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.026200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.019400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.020700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.016200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.019100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1284, training_loss=0.5562437633719771, metrics={'train_runtime': 12822.8426, 'train_samples_per_second': 0.798, 'train_steps_per_second': 0.1, 'total_flos': 118853741445120.0, 'train_loss': 0.5562437633719771, 'epoch': 6.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Entra√Ænement\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 7. Sauvegarder le mod√®le\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflan-t5-finetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflan-t5-finetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# 7. Sauvegarder le mod√®le\n",
    "model.save_pretrained(\"flan-t5-finetuned\")\n",
    "tokenizer.save_pretrained(\"flan-t5-finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Exemple de pr√©diction\n",
    "def predict_abbreviation(abbr):\n",
    "    inputs = tokenizer(abbr, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majoration pour visite a domicile justifiee par des criteres environnementaux\n",
      "Actes de prothese dentaire\n",
      "Acte dI.R.M\n",
      "Indemnite speciale de derangement\n"
     ]
    }
   ],
   "source": [
    "# Exemple :\n",
    "print(predict_abbreviation(\"MDE\"))\n",
    "print(predict_abbreviation(\"PRO\"))     \n",
    "print(predict_abbreviation(\"I\"))    \n",
    "print(predict_abbreviation(\"ISD\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', '0-data_collection-2.ipynb', '042ef63e-ccb0-4397-8b32-081b725badcc.png', '1-DataPreparation_2 & Modeling Competency.ipynb', \"1.1 Cours √©thique de l'IA version PDF.pdf\", '1.2 QCU - Corrig√© R√©vision g√©n√©rale (2).docx', '1.Le storytelling (1).pdf', '1.Le storytelling.pdf', '12. Spring Data JPA-Affectations.pdf', '1733824233566emploi.pdf', '1a9a8acc-a134-4e18-8a7e-303efd6490c9.png', '1cf68725-5802-4750-bf3e-b31a982cdb79.png', '1ff96834-e95d-4fef-a57d-b3a5d884eca8 (1).png', '2.2- MakeFile.pdf', '202410221313 (1).zip', '2425-Chapitre3 crypto.pptx', '2425Lab1 analyseVuln√©rabilit√©s (1).docx', '2425Lab1 analyseVuln√©rabilit√©s (2).docx', '2425Lab1 analyseVuln√©rabilit√©s.docx', '346147548_774684790760395_2473751248463672525_n.jpg', '346148610_268323265644397_1876516999364043878_n.jpg', '38604213.pdf', '39145479.pdf', '3iL_ingenieurs_29012025 (1).pdf', '3iL_ingenieurs_29012025.pdf', '4. La m√©thode DESC.pdf', '467657185_27835173222763215_1399687098017544230_n.mp4', '478945235_954594933451904_1061033755854248524_n.jpg', '5-JAX-RS (1).pdf', '5-JAX-RS.pdf', '6-GraphQL (1).pdf', '6-GraphQL.pdf', '626209397-chap4-MapReduce-1.pdf', '638876792-KNN.pdf', '698858921-Examen-SOA-QCM-Etude-Cas.pdf', '7-JAX-WS.pdf', '704739136-pfe-master-Data-science-and-big-data-Ouzidan-Badr-pfe.pdf', '785531129-Service-Web-REST-1.pdf', '8. Sujets oraux pour la simulation du B2.pdf', '9-1-TP 6 Mesure des d√©pendances lin√©aires.R', 'A2_mapreduce.pdf', 'a3leh.txt', 'Abalone.csv', 'Abreviations_Sans_Tarifs_Organise.txt', 'Accompagnement & Parrainage-3iL.pdf', 'ACP.ipynb', 'Activ4.rar', 'ai4i 2020 predictive maintenance dataset.zip', 'Airline Loyalty Analysis (1).pbix', 'Airline Loyalty Analysis.pbix', 'AirPassengers.csv', 'AirportManagement-Part1Correction (1).pdf', 'AirportManagement-Part2 (1).pdf', 'AirportManagementS4.rar', 'AirQuality (1).txt', 'AirQuality.txt', 'Anaconda3-2024.10-1-Windows-x86_64.exe', 'Ancien EXAMEN SOA.pdf', 'annotation.xlsx', 'ANN_TensorFlow.ipynb', 'anova-regression-correlation-analysis-a-portfolio-of-work-in-statistical-techniques-with-SPSS (1).pdf', 'anova-regression-correlation-analysis-a-portfolio-of-work-in-statistical-techniques-with-SPSS.pdf', 'Anova-Regression-Correlation-analysis.docx', 'AnyDesk.exe', 'app.py', 'Application 2-4DS7 (1).R', 'Application 2-4DS7.R', 'appsettings.json', 'ara.traineddata', 'artiste (1).txt', 'artiste.ctl', 'artiste.txt', 'artisteD.dsc', 'Atelier - Agregation (1).rar', 'Atelier - Agregation.rar', 'Atelier 2-Mapreduce (1).pdf', 'Atelier 2-Mapreduce.pdf', \"Atelier 3 - Creation d'un Makefile et la fonction main.pdf\", 'Atelier-Chap2-Langage C#-S1 (1).pdf', 'Atelier-Chap2-Langage C#-S1 (2).pdf', 'Atelier-Chap2-Langage C#-S1-Sln', 'Atelier-Chap2-Langage C#-S1-Sln.zip', 'Atelier-Chap2-Langage C#-S1.pdf', 'Atelier-Chap2-Langage C#-S2-Sln', 'Atelier-Chap2-Langage C#-S2-Sln.zip', 'Atelier-Chap2-Langage C#-S3-Sln (1)', 'Atelier-Chap2-Langage C#-S3-Sln (1).zip', 'Atelier-Chap2-Langage C#-S3-Sln (2)', 'Atelier-Chap2-Langage C#-S3-Sln (2).zip', 'Atelier-Chap2-Langage C#-S3-Sln.zip', 'Atelier-Chap2-Langage C#-S3.pdf', 'Atelier-Chap3-EntityFramework Core-S1.pdf', 'Atelier-Chap3-EntityFramework Core-S3-Sln.zip', 'Atelier-Chap3-EntityFramework Core-S4-Sln', 'Atelier-Chap3-EntityFramework Core-S4-Sln (1).zip', 'Atelier-Chap3-EntityFramework Core-S4-Sln.zip', 'Atelier-Chap4-Principes, patrons et services-S1.pdf', 'Atelier-Chap4-Principes, patrons et services-S2-Sln', 'Atelier-Chap4-Principes, patrons et services-S2-Sln.zip', 'Atelier-Chap5-Asp.Net Core MVC-S1.pdf', 'Atelier-Chap5-Asp.Net Core MVC-S3-Sln', 'Atelier-Chap5-Asp.Net Core MVC-S3-Sln.zip', 'Atelier-Consommation avec SoapUI.pdf', 'atelier1.rar', 'atelier3.rar', 'Atelier3_BigData.docx', 'atelier4.rar', 'Atelier6 DBA.pdf', 'Atelier_Hive.zip', 'Attestationdestage.pdf', 'bensalah_ons_4ds7.sql', 'bf291991-cea4-4b8c-8846-b91f638eaf68.htm', 'Bi (1).pdf', 'bi projet.pbix', 'BI.drawio.png', 'Bienvenue_dans_Colaboratory.ipynb', 'BIG-DATA.pdf', 'Black and Red Geometric Technology Keynote Presentation.pdf', 'bo.txt', 'BO2.ipynb', 'boston_house_prices (1).prn', 'Brain Tumor Data Set.zip', 'bulletin', 'bulletin.rar', 'bulletin_divise', 'bulletin_divise.rar', 'Calendrier 2425_1234 ann√©e  (1).pdf', 'Calendrier 2425_1234 ann√©e .pdf', 'CamScanner 27-09-2024 16.40.pdf', 'Cars (1).txt', 'Cars.txt', 'certifi-2022.12.7-py3-none-any.whl', 'CH1_Architecture du Serveur Oracle.pdf', 'CH2_Structures de stockage (1).pdf', 'CH2_Structures de stockage (2).pdf', 'CH2_Structures de stockage.pdf', 'ch3_hyper-parameters.pdf', 'CH3_S√©curit√© utilisateur.pdf', 'CH4_S√©curit√© et audit.pdf', 'CH5_D√©placement des donn√©es.pdf', 'ch5_Hive.pdf', 'challenges and solutions (1).pptx', 'challenges and solutions.pptx', 'Chap-3_ La R√©cursivit√©.pdf', 'Chap-4 Diviser pour r√©gner.pdf', 'Chap1 (1).zip', 'Chap1.zip', 'Chap2 (1).7z', 'Chap2.7z', 'Chap2.zip', 'Chap2_Hadoop.pdf', 'Chap3.7z', 'Chap3.zip', 'chap4_MapReduce (1).pdf', 'chap4_MapReduce.pdf', 'Chap5', 'Chap5.zip', 'chapitre 4- Agregation.pdf', \"Chapitre II_ Fonctions geÃÅneÃÅratrice et caracteÃÅristique d'une variable aleÃÅatoire reÃÅelle.pdf\", 'chapitre1-Outils de probabilit√©s.pdf', 'chapitre1.pdf', 'chapitre1_Outils fondamentaux pour les probabiliteÃÅs.pdf', 'chapitre2 (1).pdf', 'Chapitre2_Plus court chemin.pdf', 'chapitre3.pdf', 'chapitre4 (2).pdf', 'chapitre5 (2).pdf', 'ChromeSetup.exe', 'churn-bigml-20.csv', 'churn-bigml-80.csv', 'churn.txt', 'Classification (1).ipynb', 'Classification.ipynb', 'ClassificationSupervisee_CIFARAtelier.ipynb', 'Classificationversion.ipynb', 'cleaning.Rmd', 'clean_label.txt', 'CNN V2.pptx', 'commandes indexation.txt', 'Commandes_Agreg.txt', 'complexite.pdf', 'complexit√© (1) (1).pdf', 'complexit√© (1) (2).pdf', 'Complexit√© (1).pdf', 'Complexit√©.pdf', 'Compte rendu SOA.pdf', 'configs.py', 'Consommation (1).txt', 'Consommation.txt', 'Copie de Data Analytics Strategy Toolkit by Slidesgo.pptx (1).pptx', 'Copie de Data Analytics Strategy Toolkit by Slidesgo.pptx.pptx', 'Copie de NOUV18-03-2025.xlsx', 'correction', 'correction DS proba_2324 (1).pdf', 'correction examen_sp_2324.pdf', 'correction serie 3.pdf', 'correction serie2 (1).pdf', 'correction serie2.pdf', 'correction s√©rie n¬∞4 (1).pdf', 'Correction TD1.pdf', 'correction travail asynchrone n¬∞5.pdf', 'correction-DBA_SR1.txt', 'correction-DS-proba_2324.pdf', 'correction-DS_2223.pdf', 'correction-r√©gression-lin√©aire.pdf', 'correction-tp1 (1).pdf', 'correction-tp1.pdf', 'correction.rar', 'correction.zip', 'CorrectionExamenActivite.rar', 'CorrectionExamenActivite.zip', 'CorrectionExamenFete', 'CorrectionExamenFete.rar', 'CorrectionExamenFete.zip', 'correction_20serie_20n_C2_B02.pdf', 'correction_dba24SP (1).sql', 'correction_dba24SP.sql', 'Correction_Examen_1.pdf', 'Correction_Examen_2.pdf', 'CORRECTION_EXAMEN_JAX SOA_Janv2014.pdf', 'correction_Mars.sql', 'correxion', 'correxion (2).rar', 'correxion-_2_', 'correxion-_2_.zip', 'correxion.rar', 'correxion.zip', 'correxionArtiste', 'Cours 3-XSD.pdf', 'Cours-Chap2-Langage C# (1).pdf', 'Cours-Chap2-Langage C# (2).pdf', 'Cours-Chap2-Langage C#.pdf', 'Cours-Chap3-EntityFramework Core (1).pdf', 'Cours-Chap3-EntityFramework Core.pdf', 'CRIME_US_CH.csv', 'crnn.py', 'crom-tunis-nomenclature-des-actes-medicaux-2010.pdf', 'cropped field.zip', 'cropped walid.zip', 'cropped_4_page_9.jpg_1798_892.png', 'cropped_4_page_9.jpg_1799_892.png', 'cropped_4_page_9.jpg_1800_892.png', 'cropped_4_page_9.jpg_1801_892.png', 'cropped_4_page_9.jpg_1802_892.png', 'cropped_4_page_9.jpg_1803_892.png', 'cropped_4_page_9.jpg_1804_892.png', 'cropped_4_page_9.jpg_1822_891.png', 'cropped_4_page_9.jpg_1823_891.png', 'cropped_4_page_9.jpg_1824_891.png', 'cropped_4_page_9.jpg_1825_891.png', 'cropped_4_page_9.jpg_1826_891.png', 'cropped_4_page_9.jpg_1827_891.png', 'cropped_4_page_9.jpg_1828_891.png', 'cropped_4_page_9.jpg_1829_891.png', 'cropped_4_page_9.jpg_1830_891.png', 'cropped_4_page_9.jpg_1831_891.png', 'cropped_4_page_9.jpg_1832_891.png', 'cropped_4_page_9.jpg_1833_891.png', 'cropped_4_page_9.jpg_1834_891.png', 'cropped_4_page_9.jpg_1835_891.png', 'cropped_4_page_9.jpg_1841_891.png', 'cropped_4_page_9.jpg_1842_891.png', 'cropped_4_page_9.jpg_1843_891.png', 'cropped_images.json', 'cropped__page_11.jpg_1826_902.png', 'cropped__page_11.jpg_1827_902.png', 'cropped__page_11.jpg_1828_902.png', 'cropped__page_11.jpg_1829_902.png', 'cropped__page_11.jpg_1829_903.png', 'cropped__page_11.jpg_1830_902.png', 'cropped__page_11.jpg_1830_903.png', 'cropped__page_11.jpg_1831_902.png', 'cropped__page_11.jpg_1831_903.png', 'cropped__page_11.jpg_1832_902.png', 'cropped__page_11.jpg_1832_903.png', 'cropped__page_11.jpg_1833_902.png', 'cropped__page_11.jpg_1834_902.png', 'cropped__page_11.jpg_1839_901.png', 'cropped__page_11.jpg_1840_901.png', 'cropped__page_11.jpg_1841_901.png', 'cropped__page_11.jpg_1841_903.png', 'cropped__page_11.jpg_1842_901.png', 'cropped__page_11.jpg_1842_903.png', 'cropped__page_11.jpg_1843_901.png', 'cropped__page_11.jpg_1843_903.png', 'cropped__page_11.jpg_1844_901.png', 'cropped__page_11.jpg_1844_903.png', 'cropped__page_11.jpg_1845_901.png', 'cropped__page_11.jpg_1845_903.png', 'cropped__page_11.jpg_1846_903.png', 'cropped__page_11.jpg_1847_903.png', 'cropped__page_11.jpg_1848_903.png', 'cropped__page_11.jpg_1849_903.png', 'cropped__page_11.jpg_1850_903.png', 'cropped__page_11.jpg_1851_903.png', 'cropped__page_11.jpg_1852_902.png', 'cropped__page_11.jpg_1852_903.png', 'cropped__page_11.jpg_1853_902.png', 'cropped__page_11.jpg_1853_903.png', 'cropped__page_11.jpg_1854_901.png', 'cropped__page_11.jpg_1854_902.png', 'cropped__page_11.jpg_1854_903.png', 'cropped__page_11.jpg_1855_901.png', 'cropped__page_11.jpg_1855_903.png', 'cropped__page_11.jpg_1856_901.png', 'cropped__page_11.jpg_1856_903.png', 'cropped__page_11.jpg_1857_901.png', 'cropped__page_11.jpg_1857_903.png', 'cropped__page_11.jpg_1858_901.png', 'cropped__page_11.jpg_1858_903.png', 'cropped__page_11.jpg_1859_901.png', 'cropped__page_11.jpg_1859_903.png', 'cropped__page_11.jpg_1860_901.png', 'cropped__page_11.jpg_1860_903.png', 'cropped__page_11.jpg_1861_901.png', 'cropped__page_11.jpg_1861_903.png', 'cropped__page_11.jpg_1862_901.png', 'cropped__page_11.jpg_1862_903.png', 'cropped__page_11.jpg_1863_901.png', 'cropped__page_11.jpg_1864_901.png', 'cropped__page_11.jpg_1865_901.png', 'cropped__page_11.jpg_1866_901.png', 'cropped__page_11.jpg_1867_901.png', 'cropped__page_11.jpg_1868_901.png', 'cropped__page_11.jpg_1869_901.png', 'cropped__page_11.jpg_1870_901.png', 'cropped__page_11.jpg_1871_901.png', 'cropped__page_11.jpg_1872_901.png', 'cropped__page_11.jpg_1873_901.png', 'Customer Flight Activity (1).csv', 'Customer Flight Activity (2).csv', 'Customer-churn.ipynb', 'customerchurn.pbix', 'CustomerChurnTelecom (1).ipynb', 'CustomerChurnTelecom (3).ipynb', 'CustomerChurnTelecom.ipynb', 'CustomerChurnTelecom_(2).ipynb', 'cv-onsbensalah.pdf', 'cv_onsbensalah.pdf', 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'DAN-main.zip', 'data', 'Data Acquisition & Understanding .pdf', 'DATA MINING - chap0. Introduction (1).pdf', 'DATA MINING - chap0. Introduction.pdf', 'DATA MINING - chap1. analyse avec acp (1).pdf', 'DATA MINING - chap1. analyse avec acp (2).pdf', 'DATA MINING - chap1. analyse avec acp.pdf', 'DATA MINING - chap2. SEGMENTATION(K-means, CAH) (1).pdf', 'DATA MINING - chap3. r√®gles associatives (1).pdf', 'DATA MINING - chap4. Arbres de D√©cision (1).pdf', 'DATA MINING - chap5. Analyse Pr√©dictive - R√©gression Lin√©aire Multiple (1).pdf', 'DATA MINING - CLASSIFICATION - Arbres de D√©cision.pdf', 'DATA MINING - REGRESSION (1).pdf', 'DATA MINING - REGRESSION.pdf', 'data-gaz-turbine (1).r', 'data-gaz-turbine.r', 'data.csv', 'data.rar', 'data.zip', 'datamining-segmentationk-meanscah-160229202831.pdf', 'dataPI 1', 'dataPI 1.zip', 'dataset', 'dataset (1).csv', 'dataset (1).json', 'dataset (1).rar', 'dataset(1)', 'dataset.csv', 'dataset.json', 'dataset.rar', 'Datasets', 'dataset_2', 'dataset_output', 'data_abbreviations.csv', 'data_abbreviations_augmented.csv', 'data_abbreviations_balanced.csv', 'data_abbreviations_complet.csv', 'data_abbreviations_duplicated.csv', 'data_abbreviations_enrichi - Copie - Copie.csv', 'data_abbreviations_enrichi - Copie.csv', 'data_abbreviations_enrichi.csv', 'data_abbreviations_enrichi_3x_final.csv', 'data_abbreviations_enrichi_clean.csv', 'data_abbreviations_reformulated_final.csv', 'data_ai.r', 'data_complet.csv', 'Data_Preparation (1).ipynb', 'Data_Preparation (2).ipynb', 'Data_Preparation.ipynb', 'DBA.txt', 'DBA_examen - janvier2024.pdf', 'DBA_examen-control_juillet_2024.pdf', 'DBA_examen-janvier2024.pdf', 'DBA_examen-juillet_2024.pdf', 'DBA_examen-juillet_2024correction-quest-de-cours.docx', 'DBA_examen-Mars2024.pdf', 'DBA_examen-octobre_2024 (1).pdf', 'DBA_examen-octobre_2024.pdf', 'decision_tree.pdf', 'DeepLearningLab.ipynb', 'DeepLearningLab_BrainTumor.ipynb', 'dependences (1).txt', 'dependences.txt', 'desktop.ini', 'Dhia bi (1).pbix', 'Dhia bi (2).pbix', 'Dhia bi.pbix', 'Dictionnaire_Anesthesie_Organise (1).txt', 'Dictionnaire_Medical_Complet_Organise.txt', 'Dictionnaire_Medical_Organise.txt', 'Discriminative models (1).ipynb', 'Discriminative_models.ipynb', 'Discriminative_models_(1).ipynb', 'Doc-.Net-1.pdf', 'doc-3.pdf', 'DOC-NET.txt', 'doc_1.pdf', 'doc_2.pdf', 'Dossier.pdf', 'Dossier_onsbensalah.pdf', 'DotNET-1.png', 'DotNET-2.png', 'doxaria', 'Doxaria.ipynb', 'doxaria.zip', 'DS proba_2324 (1).pdf', 'DS proba_2324.pdf', 'D√©couverte des SW √©tendus (1) (1) (1).pdf', 'D√©couverte des SW √©tendus (1) (1).pdf', 'echantillonnage-calcul-des-probabilites.pdf', 'Elections_Tunisie_2024.sql', 'electronics-13-01314-v3.pdf', 'EmoDB.zip', 'EmoDeepLearning.ipynb', 'emp.dat', 'eng.traineddata', 'Esprit.xlsx', 'Exam- SI - 2024 - SR - corr.pdf', 'Exam-fleuriste.docx', 'Exam-SI-Semestre1-22-corr.pdf', 'Exam-SI-Semestre2-22-corr.pdf', 'exam.sql', 'Exam18_Exo4_page1.jpg', 'Exam18_Exo4_page2.jpg', 'Exam18_Exo4_page3.jpg', 'Exam18_Exo4_page4.jpg', 'Examen 14-15.pdf', 'Examen Beaut√©.pdf', 'Examen Corrig√© 1.pdf', 'Examen G&A 2018.pdf', 'EXAMEN Scrum.pdf', 'Examen-1-Sln', 'Examen-1-Sln.7z', 'Examen-1-Sln.zip', 'Examen-1.pdf', 'Examen-2-Sln', 'Examen-2-Sln.zip', 'Examen-2.pdf', 'Examen-3 (1).pdf', 'Examen-3.pdf', 'Examen-4 (1).pdf', 'Examen-4.pdf', 'Examen-GestionBibliotheque.pdf', 'Examen-ISEA-190517.pdf', 'Examen-Prestation.docx', 'Examen-session-principale-VF.pdf', 'Examen-SOA-2022 (3) (1).pdf', 'Examen-SOA-2022 (3).pdf', 'ExamenArtiste.pdf', 'ExamenBeauty (1)', 'ExamenBeauty.rar', 'ExamenBeauty.zip', 'ExamenFete (1).pdf', 'ExamenFete.pdf', 'ExamenSAR-21-22-S2-Correction (2).docx', 'ExamenSAR-21-22-S2-V1 (1).pdf', 'ExamenSAR-21-22-S2-V1.pdf', 'ExamenSAR-21-22-SR (1).pdf', 'ExamenSAR-21-22-SR - Correction.doc', 'examensp_2324.pdf', 'Examen_3.pdf', 'Examen_ERPBI_SP_2324_Finale_1.pdf', 'examen_rattrapage_2324.pdf', 'Examen_SOA.JAX.pdf', 'EXAMEN_SOA.pdf', 'Examen_SP_BDA_DS_2324_Finale.pdf', 'ExamJanvier2013_Corrige.pdf', 'exam_RO.pdf', 'Explainable_Artificial_Intelligence_for_Predictive_Maintenance_Applications (1).pdf', 'Explainable_Artificial_Intelligence_for_Predictive_Maintenance_Applications.pdf', 'extracted_data.json', 'extraits_images.zip', 'ezyZipp.zip', 'Fascicule 2.docx', 'Fascicule 3 2425 (1).docx', 'Fascicule 3 2425.docx', 'Fasicule 1.docx', 'Fiche suivi projet.pdf', 'Fiche_Revision_Project_Management_B2.pdf', 'Final Project SDS 322E.pdf', 'FINALE__PAPER__1-1 (1).ipynb', 'FINALE__PAPER__1-1.ipynb', 'First Lab.ipynb', 'flan-t5-finetuned', 'flan_t5_abbr_model', 'FLP (1).docx', 'FLP (2).docx', 'FLP.docx', 'FLP.pdf', 'formatted', 'fra.traineddata', 'from_image_names_labeled.txt', 'Futur Ville (1).pdf', 'Futur Ville.pdf', 'gcapi.dll', 'Geek_data.ipynb', 'Git-2.48.1-64-bit.exe', 'gitDoxaria', 'grammar presentations.eml', 'GraphQlLogement(Etudiants).rar', 'Grille Validation MLOps.pdf', 'Guide_Complet_Oral_Gestion_Projet_B2.pdf', 'Guide_Oral_Gestion_Projet_B2.mp3', 'handwritten_only.zip', 'ID-A-unique-number-for-each-record (1).txt', 'ID-A-unique-number-for-each-record.txt', 'ideaIU-2024.3.2.1.exe', 'Images_extraites_par_OCR.csv', 'InfrastructureExtensions (1).cs', 'InfrastructureExtensions.cs', 'initiation-R.pdf', 'installation mongo+tuto (1) (1).rar', 'installation mongo+tuto (1).rar', 'IntMedia.txt', 'Intoduction √† larchitecture  SOA (1).pdf', 'Intoduction √† larchitecture  SOA (2).pdf', 'Intoduction √† larchitecture  SOA.pdf', 'Intro & Chapitre 1 DPI (Intro et r√®gles g√©n√©rales).pdf', 'Intro & Chapitre 1 DPI Version PDF.pdf', 'Introduction aux syst√©mes bas√©es sur le Web (1) (1).pptx', 'Introduction Initiation BI.pdf', 'Introduction_MongoDB.pdf', 'jdk-17.0.12_windows-x64_bin.exe', 'jios-1599.pdf', 'Journal de Stage stages obligatoires (3).pdf', 'Journaldestage.pdf', 'kaggle.json', 'kali-linux-2025.1a-installer-amd64.iso', 'kali-linux-2025.1a-vmware-amd64', 'kali-linux-2025.1a-vmware-amd64.7z', 'Lab1_ANN_numpy (1).ipynb', 'Lab1_ANN_numpy.ipynb', 'Lab2_hyperparmeter_tuning (1).ipynb', 'Lab2_hyperparmeter_tuning.ipynb', 'Lab2_optimisation_regularisation.ipynb', 'label.txt', 'labelImg-master', 'labelImg-master.zip', 'Leadership (1).pptx', 'Leadership.pptx', 'Les styles de gestion conflits selon Thomas Kilman.pdf', 'logs', 'Machine Learning _ classification (ADD,R√©gression Logistique, SVM, KNN).pdf', 'Machine learning.pdf', 'Machine-Learning-_-classification-ADDR√©gression-Logistique-SVM-KNN.pdf', 'machinelearning (1).ipynb', 'machinelearning.ipynb', 'main (1).py', 'main (2).py', 'main (3).py', 'main (4).py', 'main.py', 'Makefile', 'mapper.py', 'mapper_maxT.py', 'Map_dsex_exam_1234_S2P1_2425_vf.pdf', 'Map_dsex_exam_1234_session_janvier_etud_23_24.pdf', 'MaxTemperatureDriver.java', 'MaxTemperatureMapper.java', 'MaxTemperatureReducer.java', 'merged_json.json', 'metadata.csv', 'metasploitable-linux-2.0.0', 'metasploitable-linux-2.0.0.zip', 'methodologies.pptx', 'MI-stats.pdf', 'MI.csv', 'MIclean.csv', 'ML- chap1. analyse avec acp (1).pdf', 'ML- chap1. analyse avec acp.pdf', 'MLops (1).ipynb', 'MLops.ipynb', 'ML_Project_Files (1).zip', 'ML_Project_Files.zip', 'model (1).py', 'model.py', 'modele_abv.pth', 'modele_document.pth', 'model_pipeline (1).py', 'model_pipeline (2).py', 'model_pipeline (3).py', 'model_pipeline.py', 'Module 1 _ Living Together.pdf', 'Module 1 _ Living Together.pptx', 'Module 2 _ Survival.pdf', 'Module 2 _ Survival.pptx', 'Module 3.pdf', 'Module 3.pptx', 'Module 5 _ food and health.pdf', 'Module 5 _ food and health.pptx', 'mon_modele.h5', 'mon_modele.keras', 'mon_modele_finetuned.keras', 'moy-2 (1).txt', 'moy-2.txt', 'MSTeams-x64.msix', 'My First Project.v3i.yolov8.rar', 'myCluster (1).ipynb', 'myCluster.ipynb', 'Myocardial Infarction Complications analysis.ipynb', 'myocardial+infarction+complications.zip', 'ner_abreviation_model', 'ner_model', 'newsletter2 (1).pdf', 'newsletter2.pdf', 'Nomenclatures_Significations.txt', 'Nomenclatures_Tarifs_Complets.txt', 'Nomenclature_Tableau_Symbole_Signification.txt', 'ocr_dataset_manager.py', 'ocr_manager.py', 'ocr_utils.py', 'OCR_workshop.pptx', 'OfficeSetup (1).exe', 'OfficeSetup.exe', 'oNS (1).pdf', 'oNS (2).pdf', 'oNS (3).pdf', 'ONS BEN SALAH.pdf', 'oNS.pdf', 'Onsbensalah-cv.pdf', 'onsbensalah_cv.pdf', 'onsbensalah_rapportdestage.pdf', 'OperaGXSetup.exe', 'OperaSetup.exe', 'ordonnance', 'ordonnance.rar', 'ozone.txt', 'PaddleOCR', 'paddleocr_manuscrit_colab (1).ipynb', 'paddleocr_manuscrit_colab_REGEN.ipynb', 'Paper Summary  Advances in handwriting recognition (1).pdf', 'Paper Summary  Advances in handwriting recognition.pdf', 'Paper Summary  How to easily do Handwriting Recognition using Machine Learning (1).pdf', 'Paper Summary  How to easily do Handwriting Recognition using Machine Learning.pdf', 'Paper Summary Advancements and Challenges in Handwritten Text Recognition A Comprehensive Survey (1).pdf', 'Paper Summary Advancements and Challenges in Handwritten Text Recognition A Comprehensive Survey (2).pdf', 'Paper Summary Advancements and Challenges in Handwritten Text Recognition A Comprehensive Survey.pdf', 'Paper1.pdf', 'Paper2.pdf', 'PAPER2_FINALE_VERSION1-1 (1).ipynb', 'PAPER2_FINALE_VERSION1-1.ipynb', 'Paper3.pdf', 'part2.sql', 'PBIDesktopSetup.exe', 'PCA.pdf', 'peche.txt', 'PFE BI--emira-Apache-Powebi.pptx', 'Phrases_Oral_Gestion_de_Projet_B2 (1).pdf', 'Phrases_Oral_Gestion_de_Projet_B2.pdf', 'Plan d_√©tudes_Option 4DS_2024-2025.pdf', 'PM Handouts.docx', 'Postman-win64-Setup.exe', 'prediction_passengers_rnn.ipynb', 'Prematures.xls', 'preprocessing_bullettin_soin.ipynb', 'presentation projer bi RH .pptx', 'Presentation-Chap3-EntityFramework Core-S3.pdf', 'Presentation-Chap3-EntityFramework Core-S4.pdf', 'Presentation-Chap4-Principes, patrons et services-S1 (1).pdf', 'Presentation-Chap4-Principes, patrons et services-S1 (2).pdf', 'Presentation-Chap4-Principes, patrons et services-S1.pdf', 'Presentation-Chap4-Principes, patrons et services-S2 (1).pdf', 'Presentation-Chap4-Principes, patrons et services-S2.pdf', 'Project-Specifications-Guide-WEEK7.pdf', 'Project_Challenges_and_Solutions.docx', 'Project_Challenges_and_Solutions.pdf', 'Project_Management_5min_Oral_B2.pdf', 'Project_Management_Oral_B2.pdf', 'projet ML.ipynb', 'PROJET-BI.txt', 'projet-ml-draft.docx', 'ProjetML.ipynb', 'projetstat (1).r', 'projetstat.r', 'projetStep1.r', 'Projet_BI.pbix', 'Prosit initial.pdf', 'Prototype', 'Prototype (2)', 'Prototype (2).zip', 'Prototype (3)', 'Prototype.zip', 'Prototype1', 'Prototype2', 'Prototype3', 'Prototype4', 'PST&B-ESPRIT-Convention-Mars-2023 (1).pdf', 'purchases.txt', 'PyTesseract.ipynb', 'PyTorch 2.6.0 Release source code', 'PyTorch 2.6.0 Release source code.zip', 'pytorch_lab.ipynb', 'QCM_PL_RO_Graphes_Janv 21_correction.pdf', 'query.iqy', 'R-4.4.1-win.exe', 'RandomSearch_Coarse_to_Fine.ipynb', 'rappelproba_fiche_1.pdf', 'rapport stage.docx', 'Rapport-projet-fin-d√©tude-2023 (1).docx', 'Rapport-projet-fin-d√©tude-2023.docx', 'rapport-SI-4.pdf', 'rapportpfe2023.docx', 'rapportstage_onsbensalah.pdf', 'RAVDESS.zip', 'RAVDESS_EMODB_DataUnderstanding.ipynb', 'README.md', 'receipt-ESE.BD-05__4DS7-TP 1.txt', 'receipt-ESE.FR-06__4DS7-Fiche suivi projet.txt', 'receipt-ESE.SI-17__4DS7-Compte rendu Classification.txt', 'Recherche et utilisation des mots-cl√©s.pdf', 'rec_handwritten.yml', 'reducer.py', 'reducer_maxT.py', 'regression-lineaire (1).r', 'regression-lineaire (2).r', 'regression-lineaire.r', 'regression-multiple (1).r', 'regression-multiple (2).r', 'regression-multiple.r', 'Relazione MLDM Coppola Fiori (1).pdf', 'Relazione MLDM Coppola Fiori (2).pdf', 'Relazione MLDM Coppola Fiori.pdf', 'Report Final project SDS.pdf', 'reseau.docx', 'Resistance (1).txt', 'Resistance.txt', 'Ressources', 'Ressources.rar', 'Ressources.zip', 'Ressources_Indexation', 'Ressources_Indexation.rar', 'Ressources_Indexation.zip', 'Resume of the papers.docx', 'Resume-Cours-Syst√®me-dexploitation-avanc√©-1 (1).docx', 'ResumeDroit.pdf', 'Resume_SAR1 (1).pdf', 'Resume_SAR1.pdf', 'reÃÅsumeÃÅ_cours-probabiliteÃÅs1.pdf', 'Rhistory', 'r√©sum√©.pdf', 'r√©sum√©SOA (1).pdf', 'r√©sum√©SOA.pdf', 'S1-4DS7.R', 'S2-4DS7.R', 'sample.txt', 'script.txt', 'Script_CRUD.txt', 'Script_TP6.sql', 'sentences.tgz', 'serie2 (1).pdf', 'serie2.pdf', 'service.txt', 'SOA-1-1.pdf', 'SOA-Correction-2022_1 (1).pdf', 'SOA-Correction-2022_1.pdf', 'SoapUI-x64-5.8.0 (1).exe', 'SoapUI-x64-5.8.0.exe', 'SOA_Partie1 (1).pdf', 'SOA_Partie1.pdf', 'sorted_documents.zip', 'speech.txt', 'Support_JWT (1).pdf', 'Support_JWT.pdf', 'Synthese', 'Synthese.zip', 'syst√®mes_d‚Äôexploitation_avanc√©1 (1).pptx', 'syst√®mes_d‚Äôexploitation_avanc√©1.pptx', 'S√©curit√© Informatique- Chapitre1 2425.pptx', 'S√©rie 1_√©lements de correction.pdf', 'S√©rie 2 Corrig√© des exercices 1 _ 2.pdf', \"S√©rie d'exercies n¬∞1 (1).pdf\", \"S√©rie d'exercies n¬∞1.pdf\", 's√©rie.pdf', 'S√©rie2_corrig√©_classroom.pdf', 's√©rie5_correction (1).pdf', 's√©rie5_correction.pdf', 't5_abbreviation_expander', 't5_abbr_expander', 'td analyse descriptive (1).pdf', 'TD1.pdf', 'TD2_correction-1.pdf', 'Technology Pitch Deck.pdf', 'Template BMC.docx', 'tesseract-ocr-w64-setup-5.5.0.20241111.exe', \"test-d'hpotheses-parametrique.pdf\", 'Test.ipynb', 'TestData.cs', 'TestDS-3-1.pdf', 'Tools and techniques.pptx', 'TP DNS 24-25.pdf', 'TP Load Balancer.pdf', 'TP MAIL 2024-2025 (1).pdf', 'TP MAIL 2024-2025.pdf', 'TP Nagios (1).pdf', 'TP Nagios.pdf', 'TP WEB_2023-2024 (1).pdf', 'TP WEB_2023-2024.pdf', 'TP-OpenLDAP.pdf', 'tp0 copie.pdf', 'TP06_DBA-2.sql', 'tp1-pr√©paration (1).pdf', 'tp1-pr√©paration (2).pdf', 'tp1-pr√©paration.pdf', 'TP1-S√©rie temporelle.pdf', 'TP1_DBA.pdf', 'TP1_DBA.sql', 'TP1_DBA.txt', 'TP1_SERIETEMP.R', 'TP2_DBA (1).pdf', 'TP2_DBA.pdf', \"TP3-Ajustement d'une s√©rie temporelle.pdf\", 'TP3_DBA (1).pdf', 'TP3_DBA.pdf', 'TP4-ANOVA_Correction.pdf', 'TP4_DBA (1).pdf', 'TP4_DBA.pdf', 'TP5-Test non param√©trique-4DS1-.R', 'TP_1 (1).pdf', 'TP_1 (2).pdf', 'TP_1 (3).pdf', 'TP_1.pdf', 'TP_2 (1).pdf', 'TP_2.pdf', 'TP_5.pdf', 'TP_annova.R', 'tp_haproxy.png', \"TRADUCTION DE L'ARABE.pdf\", 'train (1).txt', 'train_cleaned.txt', 'train_enriched.txt', 'train_enriched_tunisia.txt', 'train_final (1).txt', 'train_final.txt', 'train_final_balanced_cleaned.txt', 'train_final_boosted.txt', 'train_final_cleaned.txt', 'train_final_corrige.txt', 'train_final_merged.txt', 'travail en equipe compl√©it√©  (1).pdf', 'travail en equipe compl√©it√©  (2).pdf', 'travail en equipe compl√©it√© .pdf', 'tsetup-x64.5.12.3.exe', 't√©l√©chargement-Photoroom.png', 'Understanding (1).pptx', 'Understanding.pptx', 'Untitled10 (1).ipynb', 'Untitled10.ipynb', 'Untitled14.ipynb', 'Untitled16.ipynb', 'Untitled18.ipynb', 'Untitled2 (4).ipynb', 'Untitled2.ipynb', 'Untitled20.ipynb', 'Untitled6 (1).ipynb', 'Untitled6.ipynb', 'Useful_Phrases_Project_Management_B2.pdf', 'utils (1).py', 'utils.py', 'valid.txt', 'valid_cleaned.txt', 'valid_final.txt', 'Ventes.xlsx', 'VersionF.pdf', 'Vert Clair et Blanc Simple Rapport de stage Pr√©sentation.pptx', 'VerticalAttentionOCR-master', 'VerticalAttentionOCR-master.zip', 'Vid√©os - Raccourci.lnk', 'VisualStudioSetup (1).exe', 'VisualStudioSetup.exe', 'vmware-workstation-pro-17-6-0-build-24238078.exe', 'volume des ventes de materiels sportif (1).txt', 'volume des ventes de materiels sportif.txt', 'Volume MSportifs.txt', 'VSCodeUserSetup-x64-1.99.2.exe', 'wampserver3.3.7_x64.exe', 'WANDER & WORK THE DIGITAL NOMAD WAY (1).pdf', 'WANDER & WORK THE DIGITAL NOMAD WAY.pdf', 'Warm up.pptx', 'WebExtensions.cs', 'wetransfer_apache-tomcat-9-0-75_2025-02-10_1517.zip', 'wetransfer_atelier-2-mapreduce-python-cloudera-5-pdf_2024-10-07_1953.zip', 'wetransfer_examen1', 'wetransfer_examen_2025-01-08_1756', 'wetransfer_examen_2025-01-08_1756.zip', 'winrar-x64-701fr.exe', 'workshop (1).bin', 'Workshop Git - Partie1.docx', 'workshop.bin', 'Workshop.ipynb', 'Workshop_Advanced_Big_Data.pdf', 'Workshop_Clustering (1) (1).ipynb', 'Workshop_Clustering (1).ipynb', 'Workshop_Clustering (3).ipynb', 'Workshop_Clustering.ipynb', 'workshop_ML_HeartAttack.ipynb', 'workshop_powerbi -partie2.pdf', 'workshop_powerbi_partie1.pdf', 'xampp-windows-x64-8.0.30-0-VS16-installer.exe', 'XML (1).pdf', 'Zoom_cm_fr5fk4Ms8t5M0uZ9vvrZo4_m6+cJoGvUd6cKLEWdFnQBWa0kj0aqHJxjIasZ@CNH0PaioHtHV7kAQ_ka0dccf44f6ac084b_.exe', '_Neo4j_tuto.txt', '__init__.py', '__pycache__', '~$reseau.docx']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir())  # Affiche les fichiers et dossiers du r√©pertoire actuel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['checkpoint-1284', 'config.json', 'generation_config.json', 'model.safetensors', 'runs', 'special_tokens_map.json', 'spiece.model', 'tokenizer.json', 'tokenizer_config.json']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"flan-t5-finetuned\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Recharge le tokenizer depuis le dossier principal\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"flan-t5-finetuned\")\n",
    "\n",
    "# Recharge le mod√®le depuis le dernier checkpoint\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"flan-t5-finetuned/checkpoint-1284\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Exemple de pr√©diction\n",
    "def predict_abbreviation(abbr):\n",
    "    inputs = tokenizer(abbr, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consultation medecin specialiste\n",
      "Majoration pour visite a domicile justifiee par des criteres environnementaux\n",
      "Actes de prothese dentaire\n",
      "Acte dI.R.M\n",
      "Indemnite speciale de derangement\n"
     ]
    }
   ],
   "source": [
    "# Exemple :\n",
    "print(predict_abbreviation(\"CS\"))\n",
    "print(predict_abbreviation(\"MDE\"))\n",
    "print(predict_abbreviation(\"PRO\"))     \n",
    "print(predict_abbreviation(\"I\"))    \n",
    "print(predict_abbreviation(\"ISD\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
