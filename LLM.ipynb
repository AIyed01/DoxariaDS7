{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6owHzNl7M7v7"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "\n",
        "training_data = []\n",
        "with open(\"training_data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        input_text, output_text = line.strip().split(\"|||\")\n",
        "        training_data.append({\"input\": input_text, \"output\": output_text})\n",
        "\n",
        "\n",
        "class LabelCorrectionDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        return {\"input\": self.data[idx][\"input\"], \"output\": self.data[idx][\"output\"]}\n",
        "\n",
        "dataset = LabelCorrectionDataset(training_data)\n",
        "\n",
        "# Tokenize data\n",
        "def tokenize_batch(batch):\n",
        "    inputs = tokenizer([b[\"input\"] for b in batch], padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    outputs = tokenizer([b[\"output\"] for b in batch], padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    return {\"input_ids\": inputs.input_ids, \"labels\": outputs.input_ids}\n",
        "\n",
        "# Fine-tune\n",
        "model.train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "for epoch in range(3):\n",
        "    for i in range(0, len(dataset), 4):\n",
        "        batch = dataset[i:i+4]\n",
        "        tokenized = tokenize_batch(batch)\n",
        "        outputs = model(input_ids=tokenized[\"input_ids\"], labels=tokenized[\"labels\"])\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        print(f\"Epoch {epoch}, Batch {i//4}, Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "model.save_pretrained(\"label_correction_model\")\n",
        "tokenizer.save_pretrained(\"label_correction_model\")"
      ]
    }
  ]
}