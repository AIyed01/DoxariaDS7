{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48533dc1-bf9f-403a-a81e-d382e9d156b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\.conda\\envs\\GPU\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 1: Imports & Pretrained Model Setup ─────────────────────────────────\n",
    "from transformers import (\n",
    "    TrOCRProcessor,\n",
    "    VisionEncoderDecoderModel,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset as HFDataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Load the pretrained TrOCR (printed) model and its processor\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "\n",
    "# Most of these are required for generation to work\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id           = processor.tokenizer.pad_token_id\n",
    "model.config.vocab_size            = model.config.decoder.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba6411d9-b9e2-467d-ad54-ea1631cb7e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          image_path      text\n",
      "0  C:\\Users\\user\\Downloads\\TrOCR_Data\\extracted\\+...    +Bilan\n",
      "1  C:\\Users\\user\\Downloads\\TrOCR_Data\\extracted\\0...      0002\n",
      "2  C:\\Users\\user\\Downloads\\TrOCR_Data\\extracted\\0...  000pA157\n",
      "3  C:\\Users\\user\\Downloads\\TrOCR_Data\\extracted\\0...  00632506\n",
      "4  C:\\Users\\user\\Downloads\\TrOCR_Data\\extracted\\0...  00737966\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 2: Build DataFrame & Split ──────────────────────────────────────────\n",
    "images_folder    = r\"C:\\Users\\user\\Downloads\\TrOCR_Data\\extracted\"\n",
    "annotations_file = r\"C:\\Users\\user\\Downloads\\TrOCR_Data\\merged_json.json\"\n",
    "\n",
    "with open(annotations_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    annotations_list = json.load(f)\n",
    "\n",
    "annotations = {\n",
    "    os.path.basename(item[\"image_path\"]): item[\"text\"]\n",
    "    for item in annotations_list\n",
    "}\n",
    "\n",
    "data = []\n",
    "for image_file in os.listdir(images_folder):\n",
    "    if image_file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "        image_path = os.path.join(images_folder, image_file)\n",
    "        label      = annotations.get(image_file, \"\")\n",
    "        data.append({\"image_path\": image_path, \"text\": label})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head())\n",
    "\n",
    "# No test set, just train/validation\n",
    "train_df, val_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "# Create Hugging Face datasets\n",
    "hf_dsets = DatasetDict({\n",
    "    \"train\":      HFDataset.from_pandas(train_df).rename_columns({\"image_path\": \"image\", \"text\": \"text\"}),\n",
    "    \"validation\": HFDataset.from_pandas(val_df).rename_columns({\"image_path\": \"image\", \"text\": \"text\"}),\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67366444-7fcf-4c67-8510-819d1a34dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 3: Preprocessing Function & Map ────────────────────────────────────\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((384, 384)),   # TrOCR backbones expect 384×384\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def preprocess_fn(example):\n",
    "    text = example.get(\"text\", \"\").strip()\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    # Load & denoise\n",
    "    img = Image.open(example[\"image\"]).convert(\"RGB\")\n",
    "    gray = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    img  = Image.fromarray(cv2.cvtColor(blur, cv2.COLOR_GRAY2RGB))\n",
    "\n",
    "    # Resize with padding\n",
    "    w, h   = img.size\n",
    "    scale  = 384 / max(w, h)\n",
    "    new_w, new_h = int(w*scale), int(h*scale)\n",
    "    img = img.resize((new_w, new_h), Image.BILINEAR)\n",
    "    canvas = Image.new(\"RGB\", (384, 384), (255,255,255))\n",
    "    canvas.paste(img, ((384-new_w)//2, (384-new_h)//2))\n",
    "    img = canvas\n",
    "\n",
    "    # Tokenize & prepare pixel‐values\n",
    "    pixel_values = processor(images=img, return_tensors=\"pt\").pixel_values[0]\n",
    "    labels = processor.tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    ).input_ids[0]\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a769daef-be89-4277-869c-3790d9d46004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5146c212fe4aaabf9aee6044245a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/744 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494d735bb0aa4bb5baa69cfbda86b616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/132 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4fb7f4f22945a0884c4a64a3b0262b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/744 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416da37a46ac46c79a7e249bc548bcc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/132 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\.conda\\envs\\GPU\\lib\\site-packages\\transformers\\trainer.py:645: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\user\\.conda\\envs\\GPU\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\.conda\\envs\\GPU\\lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='579' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [579/920 13:48:44 < 8:09:46, 0.01 it/s, Epoch 12.43/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.546263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.460259</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.624455</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.436360</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.129700</td>\n",
       "      <td>2.464883</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\.conda\\envs\\GPU\\lib\\site-packages\\transformers\\generation\\utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\.conda\\envs\\GPU\\lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "C:\\Users\\user\\.conda\\envs\\GPU\\lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 85\u001b[0m\n\u001b[0;32m     71\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[0;32m     72\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     73\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m     },\n\u001b[0;32m     82\u001b[0m )\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# 5) Launch training\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# 6) Save final model & processor\u001b[39;00m\n\u001b[0;32m     88\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./trocr-large-finetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\site-packages\\transformers\\trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1659\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1661\u001b[0m )\n\u001b[1;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\site-packages\\transformers\\trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1927\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1928\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1929\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1932\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1933\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1934\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1935\u001b[0m ):\n\u001b[0;32m   1936\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1937\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\site-packages\\transformers\\trainer.py:2709\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2706\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m   2708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_grad_scaling:\n\u001b[1;32m-> 2709\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2710\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_apex:\n\u001b[0;32m   2711\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mscale_loss(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer) \u001b[38;5;28;01mas\u001b[39;00m scaled_loss:\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Custom Trainer Class\n",
    "class CustomTrainer(Seq2SeqTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs[\"labels\"]\n",
    "        pixel_values = inputs[\"pixel_values\"]\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Ensure the datasets are processed correctly and the columns are named as expected\n",
    "train_dataset = hf_dsets[\"train\"].map(preprocess_fn, remove_columns=hf_dsets[\"train\"].column_names, batched=False)\n",
    "val_dataset = hf_dsets[\"validation\"].map(preprocess_fn, remove_columns=hf_dsets[\"validation\"].column_names, batched=False)\n",
    "\n",
    "# Remove None values from the dataset\n",
    "train_dataset = train_dataset.filter(lambda ex: ex is not None)\n",
    "val_dataset = val_dataset.filter(lambda ex: ex is not None)\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load the WER and CER metrics\n",
    "wer = evaluate.load(\"wer\")\n",
    "cer = evaluate.load(\"cer\")\n",
    "\n",
    "def compute_metrics_fn(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    # If logits are raw, take argmax to get predicted token IDs\n",
    "    if isinstance(logits, tuple):  # safety check\n",
    "        logits = logits[0]\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Replace -100 in labels before decoding\n",
    "    labels = np.where(labels == -100, processor.tokenizer.pad_token_id, labels)\n",
    "\n",
    "    # Decode\n",
    "    pred_str = processor.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute WER and CER\n",
    "    wer_score = wer.compute(predictions=pred_str, references=label_str)\n",
    "    cer_score = cer.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\n",
    "        \"wer\": wer_score,\n",
    "        \"cer\": cer_score\n",
    "    }\n",
    "\n",
    "# 3) Set up training arguments with tuned hyperparams\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./trocr-large-finetuned\",\n",
    "    per_device_train_batch_size=4,           \n",
    "    gradient_accumulation_steps=4,           \n",
    "    learning_rate=5e-5,                      \n",
    "    weight_decay=0.01,                       \n",
    "    num_train_epochs=20,                     \n",
    "    warmup_ratio=0.1,                        \n",
    "    fp16=True,                               \n",
    "    evaluation_strategy=\"steps\",             \n",
    "    eval_steps=100,                          \n",
    "    save_strategy=\"steps\",                   \n",
    "    save_steps=250,                          \n",
    "    save_total_limit=3,\n",
    "    predict_with_generate=True,              \n",
    "    generation_max_length=128,               \n",
    "    label_smoothing_factor=0.0,              \n",
    ")\n",
    "\n",
    "# 4) Instantiate the trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    compute_metrics=compute_metrics_fn,\n",
    "    data_collator=lambda batch: {\n",
    "        \"pixel_values\": torch.stack([torch.tensor(x[\"pixel_values\"]) for x in batch]),\n",
    "        \"labels\": torch.stack([torch.tensor(x[\"labels\"]) for x in batch]),\n",
    "    },\n",
    ")\n",
    "\n",
    "# 5) Launch training\n",
    "trainer.train()\n",
    "\n",
    "# 6) Save final model & processor\n",
    "model.save_pretrained(\"./trocr-large-finetuned\")\n",
    "processor.save_pretrained(\"./trocr-large-finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef772325-4fdf-4e8c-b61c-f65327e0c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "\n",
    "training_data = []\n",
    "with open(\"training_data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        input_text, output_text = line.strip().split(\"|||\")\n",
    "        training_data.append({\"input\": input_text, \"output\": output_text})\n",
    "\n",
    "\n",
    "class LabelCorrectionDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input\": self.data[idx][\"input\"], \"output\": self.data[idx][\"output\"]}\n",
    "\n",
    "dataset = LabelCorrectionDataset(training_data)\n",
    "\n",
    "# Tokenize data\n",
    "def tokenize_batch(batch):\n",
    "    inputs = tokenizer([b[\"input\"] for b in batch], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = tokenizer([b[\"output\"] for b in batch], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    return {\"input_ids\": inputs.input_ids, \"labels\": outputs.input_ids}\n",
    "\n",
    "# Create DataLoader to handle batching\n",
    "dataloader = DataLoader(dataset, batch_size=4, collate_fn=tokenize_batch)\n",
    "\n",
    "# Fine-tune\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "for epoch in range(3):\n",
    "    for batch_idx, tokenized in enumerate(dataloader):\n",
    "        # Get tokenized input and labels\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        labels = tokenized[\"labels\"]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "\n",
    "model.save_pretrained(\"label_correction_model\")\n",
    "tokenizer.save_pretrained(\"label_correction_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efa5ebb-42b6-4dc2-85a8-4ee28271be78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8e8693-6b2b-445f-a05f-41a884731ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1411b195-5bbf-490b-9838-7f8d63f0510c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c6760-2f11-450d-b299-131154bd2cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef733ce-edc0-4724-956a-451b53aa2c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1388c7b1-f54d-4517-8d16-c002e7059031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e811c3-c948-41a3-87ab-51fe0e17e676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a980c5d6-57c3-4002-af7c-9c08266296ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b8cd4-8b72-41dd-b286-0a05dd620bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trocr_model = VisionEncoderDecoderModel.from_pretrained(\"./trocr-finetuned\")\n",
    "trocr_processor = TrOCRProcessor.from_pretrained(\"./trocr-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e314c6-3e71-4cfa-b9be-7cbfa0151a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle YOLOv8\n",
    "yolo_model = YOLO(r\"C:\\Users\\user\\Downloads\\train6\\weights\\best.pt\")\n",
    "\n",
    "# Charger l'image\n",
    "image_path =  r\"C:\\Users\\user\\Downloads\\dataset\\test\\Bulletin_de_soin\\5246--6555004--20230914_page_0.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Conversion pour Matplotlib\n",
    "\n",
    "# Détection avec un seuil de confiance bas\n",
    "results = yolo_model(image_path, conf=0.05)\n",
    "\n",
    "# Paramètres personnalisés pour les boîtes\n",
    "box_alpha = 0.2  # Transparence du remplissage\n",
    "line_width = 1    # Épaisseur des contours\n",
    "font_scale = 0.6  # Taille du texte\n",
    "text_color = (255, 0, 0)  # Couleur du texte (bleu)\n",
    "box_color = (0, 255, 0)   # Couleur des boîtes (vert)\n",
    "\n",
    "# Créer une copie pour l'annotation\n",
    "annotated_image = image.copy()\n",
    "# Extraire les coordonnées, scores et classes\n",
    "boxes = []\n",
    "scores = []\n",
    "classes = []\n",
    "\n",
    "for result in results:\n",
    "    for box in result.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        boxes.append([x1, y1, x2 - x1, y2 - y1])  # Format [x, y, w, h]\n",
    "        scores.append(float(box.conf[0]))\n",
    "        classes.append(int(box.cls))\n",
    "\n",
    "# Paramètres NMS\n",
    "nms_threshold = 0.4  # Ajustable\n",
    "confidence_threshold = 0.3  # Ne garder que les détections pertinentes\n",
    "\n",
    "# Appliquer NMS\n",
    "indices = cv2.dnn.NMSBoxes(boxes, scores, confidence_threshold, nms_threshold)\n",
    "indices = [i[0] if isinstance(i, (list, tuple, np.ndarray)) else i for i in indices]\n",
    "\n",
    "# Annoter l’image uniquement avec les boîtes gardées\n",
    "annotated_image = image.copy()\n",
    "\n",
    "for i in indices:\n",
    "    x, y, w, h = boxes[i]\n",
    "    x2, y2 = x + w, y + h\n",
    "    label = f\"{result.names[classes[i]]} {scores[i]:.2f}\"\n",
    "\n",
    "    overlay = annotated_image.copy()\n",
    "    cv2.rectangle(overlay, (x, y), (x2, y2), box_color, -1)\n",
    "    annotated_image = cv2.addWeighted(overlay, box_alpha, annotated_image, 1 - box_alpha, 0)\n",
    "    cv2.rectangle(annotated_image, (x, y), (x2, y2), box_color, line_width)\n",
    "    cv2.putText(annotated_image, label, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, font_scale, text_color, 1)\n",
    "\n",
    "\n",
    "# Affichage\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(annotated_image)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Enregistrer le résultat si besoin\n",
    "output_path = \"detection_result.jpg\"\n",
    "cv2.imwrite(output_path, cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "print(f\"Résultat sauvegardé sous : {output_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7146256-95dc-4bb5-a6ca-071948fee357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "for result in results:\n",
    "    for i, box in enumerate(result.boxes):\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "        image_pil = Image.fromarray(image)\n",
    "        cropped = image_pil.crop((x1, y1, x2, y2))\n",
    "\n",
    "        if np.array(cropped).size == 0:\n",
    "            continue\n",
    "\n",
    "        # Préparation pour le modèle TrOCR\n",
    "        inputs = trocr_processor(images=cropped, return_tensors=\"pt\").pixel_values\n",
    "        inputs = inputs.to(trocr_model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = trocr_model.generate(inputs)\n",
    "\n",
    "        text = trocr_processor.batch_decode(output, skip_special_tokens=True)[0]\n",
    "        text = text.strip() if text.strip() != \"\" else \"No text detected\"\n",
    "\n",
    "        plt.figure()\n",
    "        plt.imshow(cropped)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"OCR Output: {text}\", fontsize=10)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cb46f5-d481-4b8e-9d47-85c0ccd681c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce9817-a565-4bdf-bdcd-aba8ebfad9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08409108-8b6f-4cb9-be72-e1fb6b05c727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
