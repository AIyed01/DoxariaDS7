{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0db5db3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "def load_images_from_folder(folder):\n",
    "    X, y = [], []\n",
    "    for label, category in enumerate(CLASSES):\n",
    "        folder_path = os.path.join(folder, category)\n",
    "        \n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Dossier non trouv√© : {folder_path}\")\n",
    "            continue\n",
    "        \n",
    "        for img_name in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "\n",
    "            if img is None:\n",
    "                print(f\"Impossible de lire l'image : {img_path}\")\n",
    "                continue\n",
    "            \n",
    "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "            img = img / 255.0  # Normalisation\n",
    "            X.append(img)\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6d0a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def build_and_train_model(X_train, y_train, X_test, y_test, img_size=224, epochs=10):\n",
    "    # Chargement du mod√®le pr√©-entra√Æn√©\n",
    "    base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(img_size, img_size, 3))\n",
    "    base_model.trainable = False  # On freeze les poids\n",
    "\n",
    "    # Ajout des couches personnalis√©es\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    output = Dense(3, activation=\"softmax\")(x)  # 3 neurones pour 3 classes\n",
    "\n",
    "    # Cr√©ation du mod√®le final\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "    # Compilation du mod√®le\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # Affichage du r√©sum√© du mod√®le\n",
    "    model.summary()\n",
    "\n",
    "    # G√©n√©ration de donn√©es augment√©es\n",
    "    datagen = ImageDataGenerator(rotation_range=15, zoom_range=0.2, horizontal_flip=True)\n",
    "\n",
    "    # Entra√Ænement du mod√®le\n",
    "    history = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        epochs=epochs)\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e78aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, model, X_test, y_test):\n",
    "    IMG_SIZE = 224  # Taille utilis√©e lors de l'entra√Ænement\n",
    "\n",
    "    # Chargement et pr√©traitement de l'image\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    img = img / 255.0  # Normalisation\n",
    "    img = np.expand_dims(img, axis=0)  # Ajouter une dimension b111111atch\n",
    "\n",
    "    # Pr√©diction\n",
    "    predictions = model.predict(img)[0]  # Liste des probabilit√©s pour chaque classe\n",
    "    class_index = np.argmax(predictions)  # R√©cup√©rer l'index de la classe avec la plus haute probabilit√©\n",
    "    confidence = predictions[class_index] * 100\n",
    "\n",
    "    # Correspondance avec les classes\n",
    "    class_labels = [\"bulletin de soin\", \"ordonnance\", \"autre document m√©dical\"]\n",
    "    class_label = class_labels[class_index]\n",
    "\n",
    "    # üîπ **Calcul de l'accuracy globale du mod√®le**\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    print(f\"Pr√©diction : {class_label} | Confiance : {confidence:.2f}%\")\n",
    "    print(f\"Accuracy globale du mod√®le sur test : {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e7bf483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_templates_from_folder(folder):\n",
    "    images = {}\n",
    "    for filename in os.listdir(folder):\n",
    "        path = os.path.join(folder, filename)\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            images[filename] = img\n",
    "    return images\n",
    "def show_image(output_image_path):\n",
    "    output_image = cv2.imread(output_image_path)\n",
    "    height, width, _ = output_image.shape\n",
    "    plt.figure(figsize=(width / 100, height / 100))  # Scale factor to control figure size\n",
    "    plt.imshow(cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def find_best_template(test_image, templates):\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints2, descriptors2 = sift.detectAndCompute(test_image, None)\n",
    "    \n",
    "    best_match = None\n",
    "    best_match_count = 0\n",
    "    best_template = None\n",
    "    \n",
    "    for filename, template in templates.items():\n",
    "        keypoints1, descriptors1 = sift.detectAndCompute(template, None)\n",
    "        if descriptors1 is None or descriptors2 is None:\n",
    "            continue\n",
    "        \n",
    "        flann = cv2.FlannBasedMatcher(dict(algorithm=1, trees=5), dict(checks=50))\n",
    "        matches = flann.knnMatch(descriptors1, descriptors2, k=2)\n",
    "        good_matches = [m for m, n in matches if m.distance < 0.7 * n.distance]\n",
    "        \n",
    "        if len(good_matches) > best_match_count:\n",
    "            best_match_count = len(good_matches)\n",
    "            best_match = template\n",
    "            best_template = filename\n",
    "    \n",
    "    return best_match, best_template\n",
    "\n",
    "def align_and_brighten_image(test_image, template, dark_threshold=50, brightness_factor=1.5, brightness_offset=30):\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints1, descriptors1 = sift.detectAndCompute(template, None)\n",
    "    keypoints2, descriptors2 = sift.detectAndCompute(test_image, None)\n",
    "    \n",
    "    flann = cv2.FlannBasedMatcher(dict(algorithm=1, trees=5), dict(checks=50))\n",
    "    matches = flann.knnMatch(descriptors1, descriptors2, k=2)\n",
    "    good_matches = [m for m, n in matches if m.distance < 0.7 * n.distance]\n",
    "    \n",
    "    if len(good_matches) >= 4:\n",
    "        src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        \n",
    "        H, _ = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 5.0)\n",
    "        h, w = template.shape\n",
    "        aligned_image = cv2.warpPerspective(test_image, H, (w, h))\n",
    "        \n",
    "        mean_brightness = np.mean(aligned_image)\n",
    "        print(f\"Mean brightness: {mean_brightness}\")\n",
    "        \n",
    "        # Apply brightness correction only for extremely dark images\n",
    "        if mean_brightness < dark_threshold:\n",
    "            aligned_image = cv2.convertScaleAbs(aligned_image, alpha=brightness_factor, beta=brightness_offset)\n",
    "            print(\"Image is extremely dark. Brightness enhanced.\")\n",
    "        else:\n",
    "            print(\"Brightness is acceptable; no enhancement applied.\")\n",
    "        \n",
    "        return aligned_image\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "925ed20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def detect_tables(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Convert to binary using adaptive thresholding\n",
    "    binary = cv2.adaptiveThreshold(\n",
    "        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 15, 4\n",
    "    )\n",
    "\n",
    "    # Apply morphological operations to enhance table structures\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
    "    morphed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Filter small noise by setting a minimum area threshold\n",
    "    filtered_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > 5000]\n",
    "\n",
    "    # Get bounding boxes\n",
    "    bounding_boxes = [cv2.boundingRect(cnt) for cnt in filtered_contours]\n",
    "    bounding_boxes = sorted(bounding_boxes, key=lambda x: x[1])  # Sort by y-coordinate\n",
    "\n",
    "    # Compute image area\n",
    "    image_area = image.shape[0] * image.shape[1]\n",
    "    \n",
    "    # Compute percentage area for each table\n",
    "    table_areas = [(w * h) / image_area * 100 for x, y, w, h in bounding_boxes]\n",
    "    \n",
    "    # Draw bounding boxes on the image\n",
    "    output_image = image.copy()\n",
    "    for x, y, w, h in bounding_boxes:\n",
    "        cv2.rectangle(output_image, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
    "    \n",
    "    # Show detected tables\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Detected Tables with Bounding Boxes\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    output_image_path = \"C:/Users/user/Downloads/detected_tables_output.jpg\"\n",
    "    cv2.imwrite(output_image_path, output_image)\n",
    "\n",
    "    return table_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82575ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_table(image_path, save_path=None):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)  # Corrected this line\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Convert to binary\n",
    "    binary = cv2.adaptiveThreshold(\n",
    "        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 15, 4\n",
    "    )\n",
    "\n",
    "    # Apply morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
    "    morphed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Filter table based on area and position\n",
    "    table_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > 10000]  # Relaxed threshold\n",
    "    \n",
    "    # Get bounding boxes\n",
    "    bounding_boxes = [cv2.boundingRect(cnt) for cnt in table_contours]\n",
    "    \n",
    "    # Sort bounding boxes by y-coordinate\n",
    "    bounding_boxes = sorted(bounding_boxes, key=lambda x: x[1])\n",
    "    \n",
    "    # Draw bounding box around detected table\n",
    "    for x, y, w, h in bounding_boxes:\n",
    "        extracted_table = image[y:y+h, x:x+w]\n",
    "        if save_path:\n",
    "            cv2.imwrite(save_path, extracted_table)\n",
    "        \n",
    "        return extracted_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eff7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finition des chemins des dossiers train et test\n",
    "DATASET_DIR = \"C:/Users/user/Downloads/dataset\"\n",
    "TRAIN_DIR = os.path.join(DATASET_DIR, \"train\")\n",
    "TEST_DIR = os.path.join(DATASET_DIR, \"test\")\n",
    "CLASSES = [\"Bulletin_de_soin\", \"Ordonnance\", \"Other\"]\n",
    "IMG_SIZE = 224  # Taille standard pour MobileNetV2\n",
    "template_folder = r\"C:\\Users\\user\\Downloads\\dataset\\train\\BCNN\"\n",
    "templates = load_templates_from_folder(template_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b47911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des images depuis train et test\n",
    "X_train, y_train = load_images_from_folder(TRAIN_DIR)\n",
    "X_test, y_test = load_images_from_folder(TEST_DIR)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "print(f\"Taille du train: {len(X_train)}\")\n",
    "print(f\"Taille du validation: {len(X_val)}\")\n",
    "print(f\"Taille du test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b0265",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = build_and_train_model(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r\"C:\\Users\\user\\Downloads\\ordtest.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30e6c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(image_path, model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8b0074",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = r\"C:\\Users\\user\\Downloads\\aligned_brightened.jpg\"\n",
    "test_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "best_template, best_template_name = find_best_template(test_image, templates)\n",
    "print(f\"Best matching template: {best_template_name}\")\n",
    "result = align_and_brighten_image(test_image, best_template)\n",
    "cv2.imwrite(output_path, result)\n",
    "print(f\"Processed image saved at: {output_path}\")\n",
    "\n",
    "show_image(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62650ea9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "table_percentages = detect_tables(output_path)\n",
    "print(\"Table Area Percentages:\", table_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bce36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import difflib\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\user\\Downloads\\medical_words.csv\")\n",
    "medical_words = set(df[\"Medical Word\"].str.lower().tolist())\n",
    "\n",
    "def correct_word(word):\n",
    "    \"\"\"\n",
    "    Check if the word is in the dictionary. If not, use fuzzy matching\n",
    "    to find the best correction.\n",
    "    \"\"\"\n",
    "    lower_word = word.lower()\n",
    "    if lower_word in medical_words:\n",
    "        return word  # Word is already correct\n",
    "\n",
    "    # Look for close matches (n=1 returns the best match)\n",
    "    matches = difflib.get_close_matches(lower_word, medical_words, n=1, cutoff=0.8)\n",
    "    if matches:\n",
    "        # Preserve capitalization: if the original word starts with uppercase,\n",
    "        # capitalize the matched word.\n",
    "        corrected = matches[0]\n",
    "        if word[0].isupper():\n",
    "            corrected = corrected.capitalize()\n",
    "        return corrected\n",
    "    else:\n",
    "        # No good match found; return the original word\n",
    "        return word\n",
    "\n",
    "def correct_text(text):\n",
    "    \"\"\"\n",
    "    Process a text string by tokenizing it and correcting each word if needed.\n",
    "    Punctuation is preserved.\n",
    "    \"\"\"\n",
    "    # Tokenize the text (words and punctuation)\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "    corrected_tokens = [\n",
    "        correct_word(token) if re.match(r'\\w+', token) else token \n",
    "        for token in tokens\n",
    "    ]\n",
    "    # Reassemble the tokens into a full string\n",
    "    corrected_text = \"\"\n",
    "    for token in corrected_tokens:\n",
    "        if re.match(r'\\w+', token):\n",
    "            # Add a space if previous character is alphanumeric\n",
    "            if corrected_text and corrected_text[-1].isalnum():\n",
    "                corrected_text += \" \" + token\n",
    "            else:\n",
    "                corrected_text += token\n",
    "        else:\n",
    "            corrected_text += token\n",
    "    return corrected_text\n",
    "\n",
    "sample_text = \"The patint had an imige of a maas that was locatd near the organ. Yess, it was clear.\"\n",
    "corrected = correct_text(sample_text)\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Corrected:\", corrected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02165074",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "# Load the image and extract text using Tesseract (French language)\n",
    "image_path = \"C:\\\\Users\\\\user\\\\Downloads\\\\dataset\\\\test\\\\Ordonnance\\\\0589--1525800--20230721_page_6.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Preprocessing: Convert to grayscale (optional)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Perform OCR with French language setting\n",
    "extracted_text = pytesseract.image_to_string(gray, lang=\"fra\")\n",
    "\n",
    "# Apply correction\n",
    "corrected_text = correct_text(extracted_text)\n",
    "\n",
    "# Print results\n",
    "print(\"Original Extracted Text:\\n\", extracted_text)\n",
    "print(\"\\nCorrected Text:\\n\", corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d8da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load correction words from dataset\n",
    "correction_dict = {}\n",
    "dataset_path = r\"C:\\Users\\user\\Downloads\\FRASIMED/CANTEMIST-FRASIMED\"  # Adjust if needed\n",
    "\n",
    "for file in os.listdir(dataset_path):\n",
    "    if file.endswith(\".txt\"):  # Assuming correction words are in .txt files\n",
    "        with open(os.path.join(dataset_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                words = line.strip().split()  \n",
    "                for word in words:\n",
    "                    correction_dict[word.lower()] = word  # Store original capitalization\n",
    "\n",
    "# Function to correct text using dataset\n",
    "def correct_text(text):\n",
    "    corrected_words = [correction_dict.get(word.lower(), word) for word in text.split()]\n",
    "    return \" \".join(corrected_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d674c1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
